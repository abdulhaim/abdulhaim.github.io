<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Marwa Abdulhai</title>

  <meta name="author" content="Marwa Abdulhai">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/bair.png">
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Marwa Abdulhai</name>
              </p>
              <p> I am a PhD student at UC Berkeley in the <a href="https://bair.berkeley.edu/"> Berkeley Artificial Intelligence Research (BAIR)</a> lab advised by <a href="https://people.eecs.berkeley.edu/~svlevine/"> Sergey Levine.</a> Previously, I did my masters and undergraduate studies at MIT, where I worked with <a href="https://www.mit.edu/~jhow/">Jonathan P. How</a>.  </p>

              <p> My research aims to build AI systems that are not only capable and intelligent, but also safe, cooperative, and aligned with human values</a>. I am currently a <a href="https://www.cooperativeai.com/post/announcing-the-2025-cooperative-ai-phd-scholars">Cooperative AI PhD Fellow</a>. I am thankful to be supported by <a href="https://openai.smapply.org/prog/agentic-ai-research-grants/">Open AI Research</a>, the <a href="https://www.quadfellowship.org/2023-quad-fellows">Quad Fellowship</a>, and the <a href="https://cltc.berkeley.edu/program/ai-policy-hub/">UC Berkeley AI Policy Hub</a>.  Specifically, I am interested in:
            <ul> 
            <li><strong>AI Safety & Ethics</strong>: Designing algorithms to evaluate and mitigate deceptive behavior and align AI behavior with human values.</li>
            <li><strong>Reinforcement Learning (RL)</strong>: Developing multi-turn RL methods for dialogue, specifically focusing on negotiation and cooperative settings.</li>
            <li><strong>Language Models & Interaction</strong>: Investigating how LLMs can better model humans to build better simulators for reinforecment learning algorithms and social science studies.</li>
          </ul>

              </p>
              <p style="text-align:center">
                <a href="mailto:marwa_abdulhai@berkeley.edu">Email</a> &nbsp/&nbsp
                <a href="https://github.com/abdulhaim/">Github</a> &nbsp/&nbsp
                <a href="https://twitter.com/marwaabdulhai">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/marwaabdulhai/">LinkedIn</a>

              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/profile_marwa.png"><img style="width:75%;max-width:75%" alt="profile photo" src="images/profile_marwa.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
<!--         <h2>News</h2>

            <ul>
                <li><b>July 2025</b>: I presented LMRL-Gym at ICML 2025.
                <li><b>January 2025</b>: I became a Cooperative AI PHD Fellow.
                <li><b>July 2024</b>: Moral Foundations for LLMs was presented at EMNLP 2024.
                <li><b>September 2022</b>: I became head GSI for CS285: Deep Reinforcement Learning at UC Berkeley.
                <li><b>September 2021</b>: I began my PhD at UC Berkeley!
                <li><b>January 2021</b>: I led a program sponsored by Helping Hand Relief & Development to teach refugee students in Jordan how to code in Java. Check out course content <a href="https://hhrd-cs.github.io/">here</a>.
                <li><b>December 2020</b>: I gave a TEDxMIT Talk on the importance of interfaith dialouge and its connection to my research. Check it out <a href="https://www.youtube.com/watch?v=31L9eYsfp6E&ab_channel=TEDxTalks">here</a>.
                <li><b>September 2020</b>: I began my Masters at MIT in EECS!

            </ul> -->
        <h2>Preprints</h2>

<!--<h3>Reinforcement Learning</h3>-->          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/consistency.png" alt="lmrl_gym" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning.</papertitle>
              <br>
              <strong>Marwa Abdulhai</strong>,
              <a href="https://www.linkedin.com/in/ryanyicheng" target="_blank">Ryan Cheng</a>,
              <a href="https://www.linkedin.com/in/donovanclay" target="_blank">Donovan Clay</a>,
              <a href="https://people.eecs.berkeley.edu/~svlevine/" target="_blank">Sergey Levine</a>,
              <a href="https://natashajaques.ai/", target="_blank">Natasha Jaques</a>.
              <br>
              <em>Pre-print.</em>
              <br>
              <a href="" target="_blank">Paper</a> 
              <p>Large Language Models (LLMs) are increasingly used to simulate human users in interactive settings such as therapy, education, and social role-play. While these simulations enable scalable training and evaluation of AI agents, off-the-shelf LLMs often drift from their assigned personas, contradict earlier statements, or abandon role-appropriate behavior. We introduce a unified framework for evaluating and improving consistency in LLM-generated dialogue, reducing inconsistency by over 55\%, resulting in more coherent, faithful, and trustworthy simulated users.</p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/motivation_rl_v3.png" alt="deception" style="border-style: none" width="200">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Deception in Dialogue: Evaluating and Mitigating Deceptive Behavior in Large Language Models.</papertitle>
                  <br>
                  <strong>Marwa Abdulhai</strong>,
                  <a href="https://www.linkedin.com/in/ryanyicheng" target="_blank">Ryan Cheng</a>,
                  <a href="https://www.linkedin.com/in/aryansh-s" target="_blank">Aryansh Shrivastava</a>,
                  <a href="https://natashajaques.ai/", target="_blank">Natasha Jaques</a>,
                  <a href="https://www.cs.ox.ac.uk/people/yarin.gal/website/", target="_blank">Yarin Gal</a>,
                  <a href="https://people.eecs.berkeley.edu/~svlevine/", target="_blank">Sergey Levine</a>.
                  <br>
                  <em>Pre-print.</em>
                  <br>
                  <a href="" target="_blank">Paper</a>
                  <p>Large Language Models (LLMs) interact with hundreds of millions of people worldwide, powering applications such as customer support, education and healthcare. However, their ability to produce deceptive outputs, whether intentionally or inadvertently, poses significant safety concerns. 
                  We systematically investigate the extent to which LLMs engage in deception within dialogue, and propose the belief misalignment metric to measure deception. Benchmarking 8 state-of-the-art models indicates that LLMs naturally exhibit deceptive behaviors 24.4% of the time, even when prompted with seemingly benign objectives. When prompted to deceive, LLMs are capable of increasing deceptiveness to 43% of turns. We further explore how to use reinforcement learning to fine-tune LLMs to reduce deceptive behaviors, leading to a 15% reduction compared to other fine-tuned models. 
                </p>
                </td>
              </tr>
            </tbody></table>

        <h2>Selected Publications</h2>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/lmrl_gym.png" alt="lmrl_gym" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models.</papertitle>
              <br>
                <strong>Marwa Abdulhai</strong>,
                <a href="https://icwhite.github.io/website/" target="_blank">Isadora White</a>,
                <a href="https://sea-snell.github.io/" target="_blank">C. Snell</a>,
                <a href="https://www.linkedin.com/in/charlesjsun/", target="_blank">C. Sun</a>,
                <a href="https://jxihong.github.io/joeyhong/", target="_blank">J. Hong</a>,
                <a href="https://yx-s-z.github.io/", target="_blank">Y. Zhai</a>,
                <a href="https://kelvinxu.github.io/", target="_blank">K. Xu</a>,
                <a href="https://people.eecs.berkeley.edu/~svlevine/", target="_blank">S. Levine</a>.
              <br>
              <em>ICLR GenAI4DM Workshop 2024; ICML 2025</em>
              <br>
              <a href="https://arxiv.org/abs/2405.19752" target="_blank">Paper</a> /
              <a href="https://github.com/abdulhaim/LMRL-Gym" target="_blank">Code</a> /
              <a href="https://lmrl-gym.github.io/" target="_blank">Website</a>
              <p>We introduce LMRL‑Gym, a benchmark and toolkit for developing reinforcement learning algorithms that operate with large language models (LLMs) across eight multi-turn dialogue and text-game tasks. This work demonstrates training LLMs to plan and act strategically, ask clarifying questions, and solve long-horizon tasks—bridging reactive generation and goal-driven interaction.</p>
            </td>
          </tr>
        </tbody></table>


    <!--<h3>LLMs for Social Science</h3>-->          


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/defining_deception.png" alt="deception" style="border-style: none" width="200">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Defining Deception in Decision Making.</papertitle>
                  <br>
                  <strong>Marwa Abdulhai</strong>,
                  <a href="https://micahcarroll.github.io/" target="_blank">Micah Carroll</a>,
                  <a href="https://justinsvegliato.com/" target="_blank">Justin Svegliato</a>,
                  <a href="https://people.eecs.berkeley.edu/~anca/", target="_blank">Anca Dragan</a>,
                  <a href="https://people.eecs.berkeley.edu/~svlevine/", target="_blank">Sergey Levine</a>.
                  <br>
                  <em>AAMAS 2024</em>
                  <br>
                  <a href="https://www.ifaamas.org/Proceedings/aamas2024/pdfs/p2111.pdf" target="_blank">Paper</a>
                  <p>This paper formalizes deception in decision-making processes and proposes principled methods to detect and evaluate deceptive trajectories in multi-agent systems, using game-theoretic and reinforcement learning approaches.</p>
                </td>
              </tr>
            </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/davinci_model.png" alt="moral foundations" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Moral Foundations of Large Language Models.</papertitle>
              <br>
              <strong>Marwa Abdulhai</strong>,
              <a href="https://scholar.google.com/citations?user=fa4EXucAAAAJ&hl=en" target="_blank">Gregory Serapio-García</a>,
              <a href="https://scholar.google.com/citations?user=UrmqI6IAAAAJ&hl=en" target="_blank">Clement Crepy</a>,
              <a href="https://www.linkedin.com/in/ddwalter/" target="_blank">Daria Valter</a>,
              <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/canny.html", target="_blank">John Canny</a>,
              <a href="https://natashajaques.ai/", target="_blank">Natasha Jaques</a>.
              <br>
              <em>AAAI-23 R2HCAI (Best Paper); EMNLP 2024</em>
              <br>
              <a href="https://arxiv.org/abs/2310.15337" target="_blank">Paper</a> /
              <a href="https://github.com/abdulhaim/moral_foundations_llms" target="_blank">Code</a> /
              <a href="https://sites.google.com/view/moral-foundations-llms" target="_blank">Website</a>
              <p>Moral foundations theory (MFT) is a psychological assessment tool that decomposes human moral reasoning into five factors, including care/harm, liberty/oppression, and sanctity/degradation. As large language models (LLMs) are trained on datasets collected from the internet, they may reflect the biases that are present in such corpora. This paper uses MFT as a lens to analyze whether popular LLMs have acquired a bias towards a particular set of moral values. We analyze known LLMs and find they exhibit particular moral foundations, and show how these relate to human moral foundations and political affiliations. We also measure the consistency of these biases, or whether they vary strongly depending on the context of how the model is prompted. Finally, we show that we can adversarially select prompts that encourage the moral to exhibit a particular set of moral foundations, and that this can affect the model's behavior on downstream tasks.</p>
            </td>
          </tr>
        </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/virtual_personas.png" alt="virtual personas" style="border-style: none" width="200">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Virtual Personas for Language Models via an Anthology of Backstories.</papertitle>
                <br>
                <a href="https://suhongmoon.github.io/" target="_blank">S. Moon*</a>,
                <strong>Marwa Abdulhai*</strong>,
                <a href="https://joshuaminwookang.github.io/" target="_blank">M. Kang*</a>,
                <a href="https://josephsuh.org/" target="_blank">J. Suh*</a>,
                <a href="https://www2.eecs.berkeley.edu/Faculty/Homepages/canny.html", target="_blank">John Canny</a>.
                <br>
                <em>EMNLP 2024</em>
                <br>
                <a href="https://arxiv.org/abs/2407.06576" target="_blank">Paper</a> /
                <a href="https://github.com/CannyLab/anthology" target="_blank">Code</a> 
                <p>Large language models (LLMs) are trained from vast repositories of text authored by millions of distinct authors, reflecting an enormous diversity of human traits. In this work, we introduce "Anthology", a method for conditioning LLMs to particular virtual personas by harnessing open-ended life narratives, which we refer to as "backstories." We show that our methodology enhances the consistency and reliability of experimental outcomes while ensuring better representation of diverse sub-populations. Across three nationally representative human surveys conducted as part of Pew Research Center's American Trends Panel (ATP), we demonstrate that Anthology achieves up to 18% improvement in matching the response distributions of human respondents and 27% improvement in consistency metrics.</p>
              </td>
            </tr>
          </tbody></table>

    <!--<h3>AI Safety</h3>--> 

        <h2>Other Publications</h2>
         
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/curiosity.png" alt="virtual personas" style="border-style: none" width="200">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward</papertitle>
                <br>
                <a href="https://simon-wan.github.io/" target="_blank">Yanming Wan</a>,
                <a href="https://www.linkedin.com/in/jiaxing-wu-187624143" target="_blank">Jiaxing Wu</a>,
                <strong>Marwa Abdulhai</strong>,
                <a href="https://scholar.google.co.il/citations?user=TrQLB1gAAAAJ&hl=iw" target="_blank">Lior Shani</a>,
                <a href="https://natashajaques.ai/", target="_blank">Natasha Jaques</a>.
                <br>
                <em>2nd Workshop on Models of Human Feedback for AI Alignment at ICML 2025.</em>
                <br>
                <a href="https://arxiv.org/abs/2504.03206" target="_blank">Paper</a> /
                <p>Effective conversational agents like large language models (LLMs) must personalize their interactions to adapt to user preferences, personalities, and attributes across diverse domains like education and healthcare. Current methods like Reinforcement Learning from Human Feedback (RLHF), often prioritize helpfulness and safety but fall short in fostering truly empathetic, adaptive, and personalized dialogues. Existing personalization approaches typically rely on extensive user history, limiting their effectiveness for new or context-limited users. To address these limitations, we propose leveraging a user model to incorporate a curiosity-based intrinsic reward into multi-turn RLHF. We show improved generalization capabilities compared to standard multi-turn RLHF, all while maintaining conversation quality. Our method offers a promising solution for creating more personalized, adaptive, and engaging conversational agents.</p>
              </td>
            </tr>
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/islam_ai_faact.png" alt="virtual personas" style="border-style: none" width="200">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Blind Faith? User Preference and Expert Assessment of AI-Generated Religious Content.</papertitle>
                <br>
                <a href="https://sites.google.com/view/sabriyaalam" target="_blank">Sabriya Alam</a>,
                <strong>Marwa Abdulhai</strong>,
                <a href="https://niloufar.org/" target="_blank">Niloufar Salehi</a>.
                <br>
                <em>FAccT 2025</em>
                <br>
                <a href="https://dl.acm.org/doi/full/10.1145/3715275.3732162" target="_blank">Paper</a> 
                <p>The increasing use of AI tools like ChatGPT for religious information-seeking among Muslims raises critical questions about the intersection of technology, faith, and expertise. This mixed-methods study investigates user and expert evaluations of AI-generated religious content, through both quantitative and qualitative analysis of user preferences and expert feedback elicited through interviews, surveys, and expert consultations. Our findings reveal a significant disconnect: despite expressing distrust in AI for religious guidance and stating a preference for scholarly answers, Muslim users overwhelmingly preferred AI-generated responses to Islamic questions in blind evaluations, favoring them in 81.3% of cases. .</p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/concordia.png" alt="virtual personas" style="border-style: none" width="200">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Evaluating Generalization Capabilities of LLM Agents in Mixed-Motive Scenarios Using Concordia</papertitle>
                <br>
                <a href="https://chandlersmith.me/" target="_blank">Chandler Smith</a>,
                <strong>Marwa Abdulhai</strong>,
                <a href="https://manfreddiaz.github.io/" target="_blank">Manfred Diaz</a>,
                [80 authors],
                <a href="https://people.csail.mit.edu/dhm/", target="_blank">D. Hadfield-Menell</a>,
                <a href="https://natashajaques.ai/", target="_blank">Natasha Jaques</a>,
                <a href="https://scholar.google.com/citations?user=n9AWbcAAAAAJ&hl=en", target="_blank">J. Hernandez-Orallo</a>.
                <a href="https://www.jzleibo.com/", target="_blank">Joel Leibo</a>.
                <br>
                <em>Pre-print</em>
                <br>
                <a href="https://www.cooperativeai.com/contests/concordia-2024" target="_blank">Contest</a> /
                <a href="https://github.com/google-deepmind/concordia" target="_blank">Code</a> 
                <p>Large language model (LLM) agents have demonstrated impressive capabilities for social interaction and are increasingly being deployed in situations where they might engage with both human and artificial agents. These interactions represent a critical frontier for LLM-based agents, yet existing evaluation methods fail to measure how well these capabilities generalize to novel social situations. In this paper, we introduce a method for evaluating the ability of LLM-based agents to cooperate in zero-shot, mixed-motive environments using Concordia, a natural language multi-agent simulation environment. This work introduces an approach to measuring human-appropriate cooperative intelligence, emphasizing an agent's ability to identify and exploit opportunities for mutual gain across diverse partners and contexts.</p>
              </td>
            </tr>
          </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cradol.png" alt="cradol" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Context-Specific Representation Abstraction for Deep Option Learning.</papertitle>
              <br>
              <strong>Marwa Abdulhai</strong>,
              <a href="https://dkkim93.github.io/", target="_blank">Dongki Kim</a>,
              <a href="https://scholar.google.com/citations?user=PK7UzAwAAAAJ&hl=en" target="_blank">Matthew Riemer</a>,
              <a href="http://www.mit.edu/~miaoliu/", target="_blank">Miao Liu</a>,
              <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-gtesauro" target="_blank">Gerald Tesauro</a>,
              <a href="https://scholar.google.com/citations?user=gX7rSCcAAAAJ&hl=en", target="_blank">Jonathan P. How</a>
              <br>
              <em>AAAI-22</em>
              <br>
              <a href="https://arxiv.org/abs/2109.09876", target="_blank">Paper</a> /
              <a href="https://github.com/cradol/cradol", target="_blank">Code</a>
              <a href="https://sites.google.com/view/cradol/home", target="_blank">Video</a>
              <p></p>
              <p></p>
              <p>
              We introduce Context-Specific Representation Abstraction for Deep Option Learning (CRADOL), a new framework that considers both temporal abstraction and context-specific representation abstraction to effectively reduce the size of the search over policy space.
              </p>
            </td>
          </tr>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/meta_mapg.png" alt="meta_mapg" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>A Policy Gradient Algorithm for Learning to Learn in Multiagent Reinforcement Learning.</papertitle>
              <br>
              <a href="https://dkkim93.github.io/", target="_blank">Dongki Kim</a>,
              <a href="http://www.mit.edu/~miaoliu/", target="_blank">Miao Liu</a>,
              <a href="https://scholar.google.com/citations?user=PK7UzAwAAAAJ&hl=en" target="_blank">Matthew Riemer</a>,
              <a href="https://scholar.google.com/citations?user=BCbAD0UAAAAJ&hl=en" target="_blank">Chuangchuang Sun</a>,
              <strong>Marwa Abdulhai</strong>,
              <a href="https://scholar.google.com/citations?user=hU-LeNEAAAAJ&hl=en" target="_blank">Golnaz Habibi</a>,
              <a href="http://acl.mit.edu/people/slcot" target="_blank">Sebastian Lopez-Cot</a>,
              <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-gtesauro" target="_blank">Gerald Tesauro</a>,
              <a href="https://scholar.google.com/citations?user=gX7rSCcAAAAJ&hl=en", target="_blank">Jonathan P. How</a>
              <br>
              <em>ICML-21, AAAI-20 Symposium</em>
              <br>
              <a href="https://arxiv.org/pdf/2011.00382.pdf", target="_blank">Paper</a> /
              <a href="https://github.com/dkkim93/meta-mapg", target="_blank">Code</a>
              <a href="https://sites.google.com/view/meta-mapg/home", target="_blank">Video</a>
              <p></p>
              <p></p>
              <p>
                We develop a novel meta-multiagent policy gradient theorem that directly accommodates for the non-stationary policy dynamics inherent to multiagent settings. Our meta-agent directly considers both an agent’s own non-stationary policy dynamics and the non-stationary policy dynamics of other agents to adapt fast.
              </p>
            </td>
          </tr>
            </td>
          </tr>
        </tbody></table>
        <h2>Teaching</h2>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr onmouseout="font_stop()" onmouseover="font_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/cs188.png" alt="planning" style="border-style: none" width="200">
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
             <papertitle>Graduate Student Instructor for CS188: Artificial Intelligence </papertitle>
            <br>
            Spring 2019
            <br>
            <a href="https://inst.eecs.berkeley.edu/~cs188/sp24/", target="_blank">Course Website</a> /
            <p></p>
            <p></p>
          </td>
        </tr>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr onmouseout="font_stop()" onmouseover="font_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/deepRL.png" alt="planning" style="border-style: none" width="200">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
           <papertitle>Head Graduate Student Instructor
           CS285 Deep Reinforcement Learning</papertitle>
          <br>
          Fall 2022
          <br>
          <a href="https://rail.eecs.berkeley.edu/deeprlcourse/", target="_blank">Course Website</a> /
          <p></p>
          <p></p>
        </td>
      </tr>
        </td>
      </tr>
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/planning.png" alt="planning" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <papertitle>Teaching Assistant for 6.141: Robotics Science & Systems</papertitle>
              <br>
              Spring 2019
              <br>
              <a href="https://github.com/mit-rss", target="_blank">Assignments</a> /
              <a href="https://medium.com/@marwaabdulhai/6-141-review-7e1d548055ab", target="_blank">Medium Article</a>
              <p></p>
              <p></p>
            </td>
          </tr>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/react.jpg" alt="react" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <papertitle>Student Instructor for MIT Refugee Action Hub</papertitle>
              <br>
              January 2019
              <br>
              <a href="https://react.mit.edu/", target="_blank">Program Page</a>
              <p></p>
              <p></p>
            </td>
          </tr>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="font_stop()" onmouseover="font_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/009graph.png" alt="009graph" style="border-style: none" width="200" >
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
               <papertitle>Lab Assistant for 6.009: Fundamentals of Programming in Python</papertitle>
              <br>
              December 2018
              <br>
              <a href="https://py.mit.edu/fall20", target="_blank">Course Page</a>
              <p></p>
              <p></p>
            </td>
          </tr>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
  Template from <a href="https://jonbarron.info/">this website</a>.
</body>

</html>